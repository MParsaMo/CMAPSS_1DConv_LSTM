{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXwdTW3SmXNQOSwQJF0eVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MParsaMo/CMAPSS_1DConv_LSTM/blob/main/CMAPSS_1DConv_LSTM_unified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing train1 files :\n",
        "from google.colab import files\n",
        "train1 = files.upload()"
      ],
      "metadata": {
        "id": "GCQORCxaJtKg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "79f7bdab-4209-455b-b77c-a6e18d9f1252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-511a9d15-137a-423e-b832-dd9a3970b31c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-511a9d15-137a-423e-b832-dd9a3970b31c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RUL_FD001.txt to RUL_FD001.txt\n",
            "Saving RUL_FD002.txt to RUL_FD002.txt\n",
            "Saving RUL_FD003.txt to RUL_FD003.txt\n",
            "Saving RUL_FD004.txt to RUL_FD004.txt\n",
            "Saving test_FD001.txt to test_FD001.txt\n",
            "Saving test_FD002.txt to test_FD002.txt\n",
            "Saving test_FD003.txt to test_FD003.txt\n",
            "Saving test_FD004.txt to test_FD004.txt\n",
            "Saving train_FD001.txt to train_FD001.txt\n",
            "Saving train_FD002.txt to train_FD002.txt\n",
            "Saving train_FD003.txt to train_FD003.txt\n",
            "Saving train_FD004.txt to train_FD004.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in the current directory\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "iwuXyUAm3cMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa99d5c-27a4-4909-e20b-a5341b3376cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'train_FD004.txt', 'train_FD001.txt', 'test_FD002.txt', 'RUL_FD003.txt', 'RUL_FD001.txt', 'test_FD004.txt', 'test_FD001.txt', 'test_FD003.txt', 'RUL_FD004.txt', 'RUL_FD002.txt', 'train_FD003.txt', 'train_FD002.txt', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing important Library.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "## Reading all the \"train_FD00[i].txt\" files as Dataframes normaly.\n",
        "\n",
        "train1=pd.read_csv('train_FD001.txt',sep=\"\\s+\",header=None)\n",
        "train2=pd.read_csv('train_FD002.txt',sep=\"\\s+\",header=None)\n",
        "train3=pd.read_csv('train_FD003.txt',sep=\"\\s+\",header=None)\n",
        "train4=pd.read_csv('train_FD004.txt',sep=\"\\s+\",header=None)\n",
        "\n",
        "## Providing the Feature names.\n",
        "\n",
        "column_names=[\"engine\", \"cycle\", \"setting1\", \"setting2\", \"setting3\", \"sensor1\", \"sensor2\", \"sensor3\", \"sensor4\", \"sensor5\", \"sensor6\", \"sensor7\", \"sensor8\", \"sensor9\", \"sensor10\", \"sensor11\", \"sensor12\", \"sensor13\", \"sensor14\", \"sensor15\", \"sensor16\", \"sensor17\", \"sensor18\", \"sensor19\", \"sensor20\", \"sensor21\"]\n",
        "\n",
        "## Assigning the Feature names to the corresponding Dataframe.\n",
        "\n",
        "train1.columns=column_names\n",
        "train2.columns=column_names\n",
        "train3.columns=column_names\n",
        "train4.columns=column_names\n",
        "\n",
        "## Unique categories of each feature in train Dataframes.\n",
        "\n",
        "print(f\"Unique categories of each feature in train1 Dataframe: {len(train1.columns)}\\n\\n{train1.nunique()}\")\n",
        "print(f\"\\nUnique categories of each feature in train2 Dataframe: {len(train2.columns)}\\n\\n{train2.nunique()}\")\n",
        "print(f\"\\nUnique categories of each feature in train3 Dataframe: {len(train3.columns)}\\n\\n{train3.nunique()}\")\n",
        "print(f\"\\nUnique categories of each feature in train4 Dataframe: {len(train4.columns)}\\n\\n{train4.nunique()}\")\n",
        "\n",
        "# Some of the feature's unique categories are 1.\n",
        "#'setting3', 'sensor1', 'sensor5', 'sensor10', 'sensor16', 'sensor18' and 'sensor19' in train1\n",
        "#'setting3', 'sensor1', 'sensor5', 'sensor16', 'sensor18', 'sensor19' in train3\n",
        "\n",
        "## Removing Useless features from all selected Dataframes.\n",
        "# These drops are based on constant values in the training sets, maintain consistency.\n",
        "train1=train1.drop(columns=[\"setting3\", \"sensor1\", \"sensor5\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"])\n",
        "train3=train3.drop(columns=[\"setting3\", \"sensor1\", \"sensor5\", \"sensor16\", \"sensor18\", \"sensor19\"])\n",
        "\n",
        "## Checking remaining Features all Dataframes.\n",
        "\n",
        "print(f\"Remaining features of 'train1' Dataframe: {len(train1.columns)}\\n\\n{train1.columns}\")\n",
        "print(f\"\\nRemaining features of 'train2' Dataframe: {len(train2.columns)}\\n\\n{train2.columns}\")\n",
        "print(f\"\\nRemaining features of 'train3' Dataframe: {len(train3.columns)}\\n\\n{train3.columns}\")\n",
        "print(f\"\\nRemaining features of 'train4' Dataframe: {len(train4.columns)}\\n\\n{train4.columns}\")\n",
        "\n",
        "## Defining the function to calculate the RUL.\n",
        "\n",
        "def Calculate_RUL(df):\n",
        "    max_cycles = df.groupby('engine')['cycle'].max()\n",
        "    merged = df.merge(max_cycles.to_frame(name='max_time_cycle'), left_on='engine',right_index=True)\n",
        "    merged[\"RUL\"] = merged[\"max_time_cycle\"] - merged['cycle']\n",
        "    merged = merged.drop(\"max_time_cycle\", axis=1)\n",
        "    return merged\n",
        "\n",
        "## Merging the calculated RUL in the train Dataframes.\n",
        "\n",
        "train1=Calculate_RUL(train1)\n",
        "train2=Calculate_RUL(train2)\n",
        "train3=Calculate_RUL(train3)\n",
        "train4=Calculate_RUL(train4)\n",
        "\n",
        "## Splitting 'train1' Dataframe for model training.\n",
        "\n",
        "X1=train1.drop(columns=['engine', 'cycle', 'setting1', 'setting2', 'sensor6', 'RUL'])\n",
        "y1=train1['RUL']\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.20, random_state=42)\n",
        "\n",
        "## Splitting 'train2' Dataframe for model training.\n",
        "# Drop columns consistent with how train2 has constant values or deemed less relevant for FD002.\n",
        "# Based on the previous error, X2 has 18 features after these drops.\n",
        "X2=train2.drop(columns=['engine', 'cycle', 'setting1', 'setting2', 'setting3', 'sensor13', 'sensor16', 'sensor19', 'RUL']) # Removed 'cycle' from drops as it's not a feature for the model\n",
        "y2=train2['RUL']\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.20, random_state=42)\n",
        "print(f\"Number of features in X_train2 before scaling: {X_train2.shape[1]}\") # Should be 18\n",
        "\n",
        "## Splitting 'train3' Dataframe for model training.\n",
        "\n",
        "X3=train3.drop(columns=['engine', 'cycle', 'setting1', 'setting2', 'sensor6', 'sensor10', 'RUL'])\n",
        "y3=train3['RUL']\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.20, random_state=42)\n",
        "\n",
        "## Splitting 'train4' Dataframe for model training.\n",
        "\n",
        "X4=train4.drop(columns=['engine', 'cycle', 'setting1', 'setting2', 'setting3', 'sensor13', 'sensor16', 'sensor19', 'RUL']) # Removed 'cycle' from drops\n",
        "y4=train4['RUL']\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4, test_size=0.20, random_state=42)\n",
        "\n",
        "## Importing RUL_FD001, RUL_FD002, RUL_FD003 and RUL_FD004.\n",
        "\n",
        "rul1=pd.read_csv(\"RUL_FD001.txt\",header=None, names=[\"RUL\"])\n",
        "rul2=pd.read_csv(\"RUL_FD002.txt\",header=None, names=[\"RUL\"])\n",
        "rul3=pd.read_csv(\"RUL_FD003.txt\",header=None, names=[\"RUL\"])\n",
        "rul4=pd.read_csv(\"RUL_FD004.txt\",header=None, names=[\"RUL\"])\n",
        "\n",
        "## Importing test datasets.\n",
        "\n",
        "test1=pd.read_csv('test_FD001.txt',sep=\"\\s+\",header=None)\n",
        "test2=pd.read_csv('test_FD002.txt',sep=\"\\s+\",header=None)\n",
        "test3=pd.read_csv('test_FD003.txt',sep=\"\\s+\",header=None)\n",
        "test4=pd.read_csv('test_FD004.txt',sep=\"\\s+\",header=None)\n",
        "\n",
        "## Assigning the Feature names to the corresponding Dataframe.\n",
        "\n",
        "test1.columns=column_names\n",
        "test2.columns=column_names\n",
        "test3.columns=column_names\n",
        "test4.columns=column_names\n",
        "\n",
        "# Initialize a scaler for each dataset's training data.\n",
        "# This ensures that test/validation data are scaled using the training set's min/max.\n",
        "scaler1 = MinMaxScaler()\n",
        "scaler2 = MinMaxScaler()\n",
        "scaler3 = MinMaxScaler()\n",
        "scaler4 = MinMaxScaler()\n",
        "\n",
        "## Scaling X_train1 and X_test1\n",
        "scaled_train1=scaler1.fit_transform(X_train1)\n",
        "scaled_test1=scaler1.transform(X_test1) # Use transform, not fit_transform\n",
        "\n",
        "## Conserve only the last occurence of each unit to match the length of test1 (Old logic - will be replaced)\n",
        "# X_valid1 = test1.groupby('engine').last().reset_index().drop(columns=[\"engine\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"])\n",
        "# X_valid1.drop(columns=['sensor1','sensor5','sensor6','sensor10','sensor16','sensor18','sensor19'], inplace=True)\n",
        "# scaled_X_valid1=scaler1.transform(X_valid1) # Use transform\n",
        "\n",
        "## Scaling X_train2 and X_test2\n",
        "scaled_train2=scaler2.fit_transform(X_train2)\n",
        "scaled_test2=scaler2.transform(X_test2) # Use transform\n",
        "print(f\"Number of features in scaled_train2: {scaled_train2.shape[1]}\") # Should be 18\n",
        "\n",
        "## Conserve only the last occurence of each unit to match the length of test2 (Old logic - will be replaced)\n",
        "# X_valid2 = test2.groupby('engine').last().reset_index().drop(columns=[\"engine\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"])\n",
        "# X_valid2.drop(columns=['sensor13','sensor16', 'sensor19'], inplace=True)\n",
        "# scaled_X_valid2=scaler2.transform(X_valid2) # Use transform\n",
        "\n",
        "## Scaling X_train3 and X_test3\n",
        "scaled_train3=scaler3.fit_transform(X_train3)\n",
        "scaled_test3=scaler3.transform(X_test3) # Use transform\n",
        "\n",
        "## Conserve only the last occurence of each unit to match the length of test3 (Old logic - will be replaced)\n",
        "# X_valid3 = test3.groupby('engine').last().reset_index().drop(columns=[\"engine\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"])\n",
        "# X_valid3.drop(columns=['sensor1','sensor5','sensor6', 'sensor10', 'sensor16','sensor18','sensor19'], inplace=True)\n",
        "# scaled_X_valid3=scaler3.transform(X_valid3) # Use transform\n",
        "\n",
        "## Scaling X_train4 and X_test4\n",
        "scaled_train4=scaler4.fit_transform(X_train4)\n",
        "scaled_test4=scaler4.transform(X_test4) # Use transform\n",
        "\n",
        "## Conserve only the last occurence of each unit to match the length of test4 (Old logic - will be replaced)\n",
        "# X_valid4 = test4.groupby('engine').last().reset_index().drop(columns=[\"engine\", \"cycle\", \"setting1\", \"setting2\", \"setting3\"])\n",
        "# X_valid4.drop(columns=['sensor13','sensor16', 'sensor19'], inplace=True)\n",
        "# scaled_X_valid4=scaler4.transform(X_valid4) # Use transform\n",
        "\n",
        "# generate a sequence for LSTM model\n",
        "def generate_sequences(data, labels, window_size):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    # Ensure data is numpy array for slicing\n",
        "    data = np.array(data)\n",
        "    # Ensure labels is a Series or DataFrame for iloc, then convert to numpy at the end\n",
        "    if not isinstance(labels, (pd.Series, pd.DataFrame)):\n",
        "        labels = pd.Series(labels) # Convert to Series if it's already a numpy array\n",
        "\n",
        "    for i in range(len(data) - window_size):\n",
        "        sequence = data[i:i+window_size]\n",
        "        target = labels.iloc[i + window_size]\n",
        "        sequences.append(sequence)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# New function to extract last 'window_size' cycles for test prediction\n",
        "# This aligns test data preparation with the sequence length the model expects.\n",
        "def get_last_sequences_for_test(df_test, window_size, features_to_extract, scaler_obj):\n",
        "    sequences = []\n",
        "    # Ensure df_test has the 'engine' column and others for grouping and extraction\n",
        "    df_test_temp = df_test.copy() # Avoid modifying original\n",
        "\n",
        "    for engine_id in df_test_temp['engine'].unique():\n",
        "        engine_data = df_test_temp[df_test_temp['engine'] == engine_id][features_to_extract]\n",
        "\n",
        "        # Check if engine_data is not empty\n",
        "        if engine_data.empty:\n",
        "            continue\n",
        "\n",
        "        # Scale the engine-specific data using the pre-fitted scaler\n",
        "        scaled_engine_data = scaler_obj.transform(engine_data)\n",
        "\n",
        "        if len(scaled_engine_data) >= window_size:\n",
        "            # Take the last 'window_size' observations\n",
        "            sequences.append(scaled_engine_data[-window_size:])\n",
        "        else:\n",
        "            # If an engine has fewer cycles than window_size, pad it\n",
        "            # Common padding strategies:\n",
        "            # 1. Replicate the first observation:\n",
        "            # padded_sequence = np.tile(scaled_engine_data[0], (window_size, 1))\n",
        "            # padded_sequence[-len(scaled_engine_data):] = scaled_engine_data\n",
        "            # 2. Replicate the last observation (more common for RUL):\n",
        "            padded_sequence = np.tile(scaled_engine_data[-1], (window_size, 1))\n",
        "            padded_sequence[:len(scaled_engine_data)] = scaled_engine_data\n",
        "            sequences.append(padded_sequence)\n",
        "            # Alternative: pad with zeros or skip this engine if window_size is strict\n",
        "    return np.array(sequences)\n",
        "\n",
        "\n",
        "# Our model building function\n",
        "def build_model(input_features, window_size=30): # Pass the number of features and window_size as argument\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size, input_features)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),  # Optional: Downsamples feature map\n",
        "        Dropout(0.3),\n",
        "\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        LSTM(32),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)  # Final regression output\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model # Return the compiled model\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "WINDOW_SIZE = 30 # Define window size globally\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, X_train, y_train, X_val=None, y_val=None, batch_size=BATCH_SIZE, epochs=EPOCHS):\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=0.00001) # Added ReduceLROnPlateau\n",
        "    ]\n",
        "\n",
        "    if X_val is not None:\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            callbacks=callbacks)\n",
        "    else:\n",
        "        history = model.fit(X_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            callbacks=callbacks)\n",
        "\n",
        "    return history\n",
        "\n",
        "# Evaluate model (modified to calculate metrics manually)\n",
        "def evaluate_model_manual(model, X, y, label=\"\"):\n",
        "    preds = model.predict(X, verbose=0) # Set verbose to 0 to suppress per-batch output\n",
        "\n",
        "    if hasattr(preds, \"numpy\"):\n",
        "        preds = preds.numpy()\n",
        "\n",
        "    preds = preds.reshape(-1) # Flatten predictions to 1D\n",
        "\n",
        "    # Ensure y is also a 1D numpy array\n",
        "    if isinstance(y, (pd.Series, pd.DataFrame)):\n",
        "        y_true = y.values.reshape(-1)\n",
        "    else:\n",
        "        y_true = y.reshape(-1)\n",
        "\n",
        "    mse = mean_squared_error(y_true, preds)\n",
        "    mae = mean_absolute_error(y_true, preds)\n",
        "    r2 = r2_score(y_true, preds) # Optional: Add R2 score\n",
        "\n",
        "    print(f\"{label} -> MSE: {mse:.3f}, MAE: {mae:.3f}, R2: {r2:.3f}\")\n",
        "\n",
        "    print(f\"{label} preds type:\", type(preds), \"shape:\", preds.shape)\n",
        "    print(f\"{label} true_y type:\", type(y_true), \"shape:\", y_true.shape)\n",
        "\n",
        "    return preds\n",
        "\n",
        "# --- Focus on FD002 Dataset ---\n",
        "\n",
        "# Determine the number of features for FD002 model input\n",
        "# X_train2 has 18 features (columns)\n",
        "fd002_input_features = X_train2.shape[1]\n",
        "print(f\"FD002 Model Input Features: {fd002_input_features}\")\n",
        "\n",
        "# Build the model for FD002\n",
        "model_fd002 = build_model(fd002_input_features, window_size=WINDOW_SIZE)\n",
        "model_fd002.summary()\n",
        "\n",
        "# Apply to FD002 Dataset and prepare sequences for training and internal validation\n",
        "X_seq_train2, y_seq_train2 = generate_sequences(scaled_train2, y_train2, window_size=WINDOW_SIZE)\n",
        "X_seq_test2, y_seq_test2 = generate_sequences(scaled_test2, y_test2, window_size=WINDOW_SIZE) # For internal validation during training\n",
        "\n",
        "# Confirm sequence shapes\n",
        "print(\"X_seq_train2 shape:\", X_seq_train2.shape)\n",
        "print(\"y_seq_train2 shape:\", y_seq_train2.shape)\n",
        "print(\"X_seq_test2 shape:\", X_seq_test2.shape)\n",
        "print(\"y_seq_test2 shape:\", y_seq_test2.shape)\n",
        "print(\"Types: X_seq_train2 - {}, y_seq_train2 - {}\".format(type(X_seq_train2), type(y_seq_train2)))\n",
        "\n",
        "# Train on FD002\n",
        "print(\"\\n--- Training Model for FD002 ---\")\n",
        "# Use X_seq_test2 and y_seq_test2 for validation during training\n",
        "history_fd002 = train_model(model_fd002, X_seq_train2, y_seq_train2, X_val=X_seq_test2, y_val=y_seq_test2)\n",
        "\n",
        "# Predict sample and print shape\n",
        "sample_pred_fd002 = model_fd002.predict(X_seq_train2[:10])\n",
        "print(\"Sample Prediction Shape (after model_fd002.predict):\", sample_pred_fd002.shape)\n",
        "\n",
        "\n",
        "# --- Prepare Test Data (FD002) for Final Evaluation ---\n",
        "# Define features to extract from the raw test2 dataframe, consistent with X2\n",
        "# These are the columns that were NOT dropped in X2, excluding 'engine', 'cycle', 'RUL'\n",
        "features_for_test2_final_eval = [col for col in column_names if col not in ['engine', 'cycle', 'setting1', 'setting2', 'setting3', 'sensor13', 'sensor16', 'sensor19']]\n",
        "\n",
        "# Use the scaler that was fitted on X_train2\n",
        "X_valid2_final_sequences = get_last_sequences_for_test(test2, window_size=WINDOW_SIZE,\n",
        "                                                         features_to_extract=features_for_test2_final_eval,\n",
        "                                                         scaler_obj=scaler2)\n",
        "y_valid2_final = rul2['RUL'].values # The true RULs for the test engines\n",
        "\n",
        "\n",
        "# Confirm shapes for final evaluation\n",
        "print(\"X_valid2_final_sequences shape (for final evaluation):\", X_valid2_final_sequences.shape)\n",
        "print(\"y_valid2_final shape (for final evaluation):\", y_valid2_final.shape)\n",
        "\n",
        "\n",
        "# Evaluate model using the manual evaluation function\n",
        "print(\"\\n--- Evaluation on FD002 ---\")\n",
        "train_preds_fd002 = evaluate_model_manual(model_fd002, X_seq_train2, y_seq_train2, label=\"Train Set (FD002)\")\n",
        "test_preds_fd002  = evaluate_model_manual(model_fd002, X_seq_test2,  y_seq_test2,  label=\"Test Set (FD002 - Internal Validation)\")\n",
        "final_valid_preds_fd002 = evaluate_model_manual(model_fd002, X_valid2_final_sequences, y_valid2_final, label=\"Final Validation Set (FD002 - RUL2)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d-JoWdyvOGay",
        "outputId": "925e0bef-b55b-485a-d8ad-cd91dead6939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique categories of each feature in train1 Dataframe: 26\n",
            "\n",
            "engine       100\n",
            "cycle        362\n",
            "setting1     158\n",
            "setting2      13\n",
            "setting3       1\n",
            "sensor1        1\n",
            "sensor2      310\n",
            "sensor3     3012\n",
            "sensor4     4051\n",
            "sensor5        1\n",
            "sensor6        2\n",
            "sensor7      513\n",
            "sensor8       53\n",
            "sensor9     6403\n",
            "sensor10       1\n",
            "sensor11     159\n",
            "sensor12     427\n",
            "sensor13      56\n",
            "sensor14    6078\n",
            "sensor15    1918\n",
            "sensor16       1\n",
            "sensor17      13\n",
            "sensor18       1\n",
            "sensor19       1\n",
            "sensor20     120\n",
            "sensor21    4745\n",
            "dtype: int64\n",
            "\n",
            "Unique categories of each feature in train2 Dataframe: 26\n",
            "\n",
            "engine        260\n",
            "cycle         378\n",
            "setting1      536\n",
            "setting2      105\n",
            "setting3        2\n",
            "sensor1         6\n",
            "sensor2      1590\n",
            "sensor3     12305\n",
            "sensor4     15411\n",
            "sensor5         6\n",
            "sensor6        14\n",
            "sensor7      2067\n",
            "sensor8       897\n",
            "sensor9     22434\n",
            "sensor10        9\n",
            "sensor11      681\n",
            "sensor12     1672\n",
            "sensor13      514\n",
            "sensor14    14905\n",
            "sensor15     8464\n",
            "sensor16        2\n",
            "sensor17       53\n",
            "sensor18        6\n",
            "sensor19        2\n",
            "sensor20      510\n",
            "sensor21    17837\n",
            "dtype: int64\n",
            "\n",
            "Unique categories of each feature in train3 Dataframe: 26\n",
            "\n",
            "engine       100\n",
            "cycle        525\n",
            "setting1     160\n",
            "setting2      14\n",
            "setting3       1\n",
            "sensor1        1\n",
            "sensor2      334\n",
            "sensor3     3358\n",
            "sensor4     4383\n",
            "sensor5        1\n",
            "sensor6       17\n",
            "sensor7     1854\n",
            "sensor8      161\n",
            "sensor9     7114\n",
            "sensor10       4\n",
            "sensor11     170\n",
            "sensor12    1772\n",
            "sensor13     163\n",
            "sensor14    6320\n",
            "sensor15    3122\n",
            "sensor16       1\n",
            "sensor17      12\n",
            "sensor18       1\n",
            "sensor19       1\n",
            "sensor20     165\n",
            "sensor21    6440\n",
            "dtype: int64\n",
            "\n",
            "Unique categories of each feature in train4 Dataframe: 26\n",
            "\n",
            "engine        249\n",
            "cycle         543\n",
            "setting1      536\n",
            "setting2      105\n",
            "setting3        2\n",
            "sensor1         6\n",
            "sensor2      1704\n",
            "sensor3     13558\n",
            "sensor4     17353\n",
            "sensor5         6\n",
            "sensor6        46\n",
            "sensor7      5926\n",
            "sensor8      1038\n",
            "sensor9     25297\n",
            "sensor10       21\n",
            "sensor11      737\n",
            "sensor12     5627\n",
            "sensor13      483\n",
            "sensor14    15938\n",
            "sensor15    11915\n",
            "sensor16        2\n",
            "sensor17       54\n",
            "sensor18        6\n",
            "sensor19        2\n",
            "sensor20      652\n",
            "sensor21    21574\n",
            "dtype: int64\n",
            "Remaining features of 'train1' Dataframe: 19\n",
            "\n",
            "Index(['engine', 'cycle', 'setting1', 'setting2', 'sensor2', 'sensor3',\n",
            "       'sensor4', 'sensor6', 'sensor7', 'sensor8', 'sensor9', 'sensor11',\n",
            "       'sensor12', 'sensor13', 'sensor14', 'sensor15', 'sensor17', 'sensor20',\n",
            "       'sensor21'],\n",
            "      dtype='object')\n",
            "\n",
            "Remaining features of 'train2' Dataframe: 26\n",
            "\n",
            "Index(['engine', 'cycle', 'setting1', 'setting2', 'setting3', 'sensor1',\n",
            "       'sensor2', 'sensor3', 'sensor4', 'sensor5', 'sensor6', 'sensor7',\n",
            "       'sensor8', 'sensor9', 'sensor10', 'sensor11', 'sensor12', 'sensor13',\n",
            "       'sensor14', 'sensor15', 'sensor16', 'sensor17', 'sensor18', 'sensor19',\n",
            "       'sensor20', 'sensor21'],\n",
            "      dtype='object')\n",
            "\n",
            "Remaining features of 'train3' Dataframe: 20\n",
            "\n",
            "Index(['engine', 'cycle', 'setting1', 'setting2', 'sensor2', 'sensor3',\n",
            "       'sensor4', 'sensor6', 'sensor7', 'sensor8', 'sensor9', 'sensor10',\n",
            "       'sensor11', 'sensor12', 'sensor13', 'sensor14', 'sensor15', 'sensor17',\n",
            "       'sensor20', 'sensor21'],\n",
            "      dtype='object')\n",
            "\n",
            "Remaining features of 'train4' Dataframe: 26\n",
            "\n",
            "Index(['engine', 'cycle', 'setting1', 'setting2', 'setting3', 'sensor1',\n",
            "       'sensor2', 'sensor3', 'sensor4', 'sensor5', 'sensor6', 'sensor7',\n",
            "       'sensor8', 'sensor9', 'sensor10', 'sensor11', 'sensor12', 'sensor13',\n",
            "       'sensor14', 'sensor15', 'sensor16', 'sensor17', 'sensor18', 'sensor19',\n",
            "       'sensor20', 'sensor21'],\n",
            "      dtype='object')\n",
            "Number of features in X_train2 before scaling: 18\n",
            "Number of features in scaled_train2: 18\n",
            "FD002 Model Input Features: 18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m3,520\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m33,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,520</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,305\u001b[0m (196.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,305</span> (196.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,177\u001b[0m (196.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,177</span> (196.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq_train2 shape: (42977, 30, 18)\n",
            "y_seq_train2 shape: (42977,)\n",
            "X_seq_test2 shape: (10722, 30, 18)\n",
            "y_seq_test2 shape: (10722,)\n",
            "Types: X_seq_train2 - <class 'numpy.ndarray'>, y_seq_train2 - <class 'numpy.ndarray'>\n",
            "\n",
            "--- Training Model for FD002 ---\n",
            "Epoch 1/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 13ms/step - loss: 10158.0811 - mae: 79.6838 - val_loss: 4694.3936 - val_mae: 56.5061 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4981.0903 - mae: 58.0094 - val_loss: 4696.5068 - val_mae: 56.4779 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 4985.1279 - mae: 57.9885 - val_loss: 4695.3838 - val_mae: 56.6377 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4960.8335 - mae: 57.8404 - val_loss: 4702.8179 - val_mae: 56.4415 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4968.1138 - mae: 58.0793 - val_loss: 4696.2505 - val_mae: 56.4806 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4946.3433 - mae: 57.7071 - val_loss: 4693.5635 - val_mae: 56.5329 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 4980.2559 - mae: 58.0655 - val_loss: 4705.6191 - val_mae: 56.4339 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4924.9624 - mae: 57.6962 - val_loss: 4699.8892 - val_mae: 56.4544 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4886.9810 - mae: 57.4404 - val_loss: 4693.4136 - val_mae: 56.5602 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 4938.6138 - mae: 57.8546 - val_loss: 4696.1743 - val_mae: 56.4815 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 4957.5493 - mae: 57.8351 - val_loss: 4698.7295 - val_mae: 56.4609 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4969.4873 - mae: 57.9362 - val_loss: 4697.7178 - val_mae: 56.4673 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4893.9976 - mae: 57.5817 - val_loss: 4700.9097 - val_mae: 56.4484 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m669/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4937.7866 - mae: 57.7176\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 4937.7100 - mae: 57.7177 - val_loss: 4693.7524 - val_mae: 56.5194 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4929.6875 - mae: 57.8746 - val_loss: 4693.6895 - val_mae: 56.5225 - learning_rate: 5.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4857.5273 - mae: 57.3545 - val_loss: 4693.4761 - val_mae: 56.5758 - learning_rate: 5.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 4932.2290 - mae: 57.8516 - val_loss: 4693.3130 - val_mae: 56.5187 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4916.4097 - mae: 57.6646 - val_loss: 4693.2896 - val_mae: 56.5278 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4957.0889 - mae: 58.0302 - val_loss: 4695.1226 - val_mae: 56.4884 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 4926.3691 - mae: 57.7900 - val_loss: 4693.4478 - val_mae: 56.5170 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 4915.5239 - mae: 57.7818 - val_loss: 4696.2979 - val_mae: 56.4697 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4860.4453 - mae: 57.3707 - val_loss: 4692.9453 - val_mae: 56.5355 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 4970.7441 - mae: 58.1149 - val_loss: 4697.8506 - val_mae: 56.4538 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 4931.9956 - mae: 57.8881 - val_loss: 4694.0688 - val_mae: 56.5094 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 4886.8340 - mae: 57.5381 - val_loss: 4695.3994 - val_mae: 56.4801 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4876.8120 - mae: 57.4716 - val_loss: 4693.8848 - val_mae: 56.4955 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m671/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4910.0112 - mae: 57.6544\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4910.0107 - mae: 57.6544 - val_loss: 4694.1401 - val_mae: 56.6184 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4867.0859 - mae: 57.4943 - val_loss: 4694.4751 - val_mae: 56.4785 - learning_rate: 2.5000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 4896.7720 - mae: 57.5282 - val_loss: 4697.1001 - val_mae: 56.4556 - learning_rate: 2.5000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4911.8994 - mae: 57.7524 - val_loss: 4692.8643 - val_mae: 56.5460 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 4891.5562 - mae: 57.6469 - val_loss: 4693.0444 - val_mae: 56.5697 - learning_rate: 2.5000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 4886.3262 - mae: 57.6815 - val_loss: 4694.6440 - val_mae: 56.5032 - learning_rate: 2.5000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 4936.1343 - mae: 57.7174 - val_loss: 4694.2949 - val_mae: 56.5243 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4928.2446 - mae: 57.6641 - val_loss: 4700.5493 - val_mae: 56.4517 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m670/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4876.5088 - mae: 57.4759\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4876.5293 - mae: 57.4760 - val_loss: 4694.9277 - val_mae: 56.5015 - learning_rate: 2.5000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4934.3970 - mae: 57.8926 - val_loss: 4696.1768 - val_mae: 56.5036 - learning_rate: 1.2500e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4894.1460 - mae: 57.6974 - val_loss: 4695.9937 - val_mae: 56.5241 - learning_rate: 1.2500e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 4886.4551 - mae: 57.5443 - val_loss: 4696.6318 - val_mae: 56.5173 - learning_rate: 1.2500e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 4855.1279 - mae: 57.2962 - val_loss: 4697.5273 - val_mae: 56.4978 - learning_rate: 1.2500e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m670/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4867.6460 - mae: 57.3402\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 4867.7349 - mae: 57.3411 - val_loss: 4698.1460 - val_mae: 56.5046 - learning_rate: 1.2500e-04\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step\n",
            "Sample Prediction Shape (after model_fd002.predict): (10, 1)\n",
            "X_valid2_final_sequences shape (for final evaluation): (259, 30, 18)\n",
            "y_valid2_final shape (for final evaluation): (259,)\n",
            "\n",
            "--- Evaluation on FD002 ---\n",
            "Train Set (FD002) -> MSE: 4806.865, MAE: 57.206, R2: 0.001\n",
            "Train Set (FD002) preds type: <class 'numpy.ndarray'> shape: (42977,)\n",
            "Train Set (FD002) true_y type: <class 'numpy.ndarray'> shape: (42977,)\n",
            "Test Set (FD002 - Internal Validation) -> MSE: 4692.864, MAE: 56.546, R2: 0.000\n",
            "Test Set (FD002 - Internal Validation) preds type: <class 'numpy.ndarray'> shape: (10722,)\n",
            "Test Set (FD002 - Internal Validation) true_y type: <class 'numpy.ndarray'> shape: (10722,)\n",
            "Final Validation Set (FD002 - RUL2) -> MSE: 3617.519, MAE: 50.964, R2: -0.251\n",
            "Final Validation Set (FD002 - RUL2) preds type: <class 'numpy.ndarray'> shape: (259,)\n",
            "Final Validation Set (FD002 - RUL2) true_y type: <class 'numpy.ndarray'> shape: (259,)\n"
          ]
        }
      ]
    }
  ]
}